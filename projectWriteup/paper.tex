\documentclass[12pt]{article}
\usepackage{e-jc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\title{An Optimized Agent for Capture The Flag Pac-Man}

\author{
Alexander L. Churchill\\
\small \texttt{achur@stanford.edu}\\
\and
Emilio Lopez\\
\small \texttt{elopez1@stanford.edu}\\
\and
Rafael Witten\\
\small \texttt{rwitten@stanford.edu}
}

\date{Mar. 8, 2011}

\begin{document}
\maketitle
\section{Introduction}
A popular and relevant field in artificial intelligence is noisy gameplay
\cite{miik}.  One fun example of this field is Capture the Flag (CTF)
Pac-Man.  In this paper, we discuss our implementation of an optimized, flexible agent to play CTF Pac-Man.

In our balanced agent, we observed that there are three main branches of
strategy in CTF Pac-Man: attacking, holding the line, and defending.  The
first attempts to eat opposing food, the second attempts to block opposing
agents from entering our side, and the third attempts to capture opposing
agents and prevent them from eating our food once they cross over.  We
built individual modules for each one of these strategies to recommend
moves.  As in many AI agents, the recommendations of these modules and the
global decision of which module to apply use a combination of minimax and
expectimax.

Our differentiator and the primary focus of our work was on more advanced
ways to model opponents and determine the value of a given game state,
as well as techniques to optimize the weights used to combine those
parameters into relevant state values.  We used opponent modeling and clever observations in addition to our noisy readouts to infer our opponents' positions.  Our various agents incorporated a wide variety of heuristics,
weighted to recommend moves.  We used a technique called Particle Swarm
Optimization (PSO) to find a strong set of weights.  PSO tends to perform
well in functions like ours that are both difficult to evaluate accurately and
impossible to analyze from a theoretical perspective \cite{carlisle}.  To integrate the specific results of PSO to new layouts, we clustered a layout at gametime with other layouts for which we had pre-computed PSO values.


\section{Inference}
The first problem we had to solve was that of determining where opposing agents were: the noisy readings from the game state left much to be desired.  For inference of opponent agent positions, we update prior probabilities based not only on our sensor readings, but also based on assumptions in our opponent model, which assumes that the opponent is at all times more likely to be moving towards our food and, therefore, places higher probabilities on positions that get them closer. Additionally, our inference module analyzes our food grid and, whenever one of our pieces of food gets eaten, updates the probabilities of the enemy agent that ate it (whichever agent moved before our agent who is currently analyzing its sensor readings). For obvious reasons, our inference module also handles the two cases in which we know an opponents' location: when we eat them, their probability mass goes back to their start position, and once they are in ``line of sight'' we update their probability distribution accordingly.

\section{Heuristics}
We thought much more about our choice of heuristics than about their weights, relying instead on automated techniques to choose optimal parameters.  Our choice of heuristics was primarily driven by attempting to think about things in ways similar to humans; in each ``mode" we evaluated the position using heuristics relevant to examining the board from the perspective of the human strategy.

Hence, we wrote a heuristic function that looked at a board and classified each of the agents, theirs and ours, as being in one of those three states.  To evaluate a position we took a weighted sum of how the three heuristics thought we were doing; thus our heuristic was considering a weighting of how well our defenders were protecting our dots, our attackers were threatening their dots and our hold the line agents were preventing them from crossing onto our side.
\subsection{Offense}
Playing offense used a number of heuristics designed to reward agents that could capture food maximally on each attack run.  We found that the most important heuristics were the number of food on the board, our distance to the nearest food, and our total distance to food items.  We also included heuristics taking into account how close we were to other agents, both opposing ghosts and teammates.  Finally, we imposed a heuristic penalty for entering cells of the board that were trapped by opposing agents.  

\subsection{Defense}
The defensive heuristics that we found most helpful were staying close to our enemies, getting them into single-exit (trapped) scenarios and ensuring that for any piece of food we were closer than our opponents.  A key limitation that we had was that losing sight of attacking enemies could be very dangerous because mutual lack of information is very helpful to attackers.  However, because we were able to deduce an exact location any time food went missing, any agent that was doing serious harm was inherently easy to track.
\subsection{Hold the Line}
Playing defense against agents who had not crossed onto our side was a very different problem than playing it against agents who had, mostly because trying to eat them was no longer a strong strategy.  Hence we hit upon the idea of separating that type of play into a ``Hold The Line" heuristic.  For the purposes of holding the line, our agents would ``match up'' against usually one but sometimes more opposing agents depending on whether we had some agents attacking.  Our agents mirror their moves, trying to stay closer to all of the squares on the border than the opponents that that agent was ``covering''.  With good inference, as is possible in a 6 agent game, it could become very hard for any opposing agents to cross the border.  

\section{Particle Swarm Optimization}
The fitness function that we were trying to maximize is, of course, how well an agent plays.  However, evaluating the performance of a strategy is tricky because it requires playing many games to determine a win percentage against some training set of agents and evaluating some measure of fitness.  Evaluating a win percentage is quite noisy and playing a single pacman game is computationally very expensive (as much as 50 minutes, although for training purposes we cutdown thinking time to make games around 5 minutes long).  Hence, when evaluating our agents we got very noisy results.

Part of the appeal of Particle Swarm Optimization (PSO) is that it has in practice been shown to be fairly robust against noise in evaluating an objective function \cite{pars}.  PSO was proposed in 1995 by \cite{originalref} for optimizing functions that are continuous but have no other useful structure.  The appeal of PSO over a Monte Carlo stochastic optimization scheme such as \cite{montecarlo} is that we have a strong suspicion that our true objective function is locally pretty smooth, which fits nicely with local search based methods like PSO.  Additionally using more particles helps us avoid local minima to which genetic algorithms are susceptible as suggested by \cite{geneticalgorithm}.  However, gradient descent is not an option because we cannot compute a first derivative and empirical gradient descent would be noisy because we cannot accurately compute the function at any one point so our empirical gradients would be very inaccurate.

\subsection{Algorithm}
We assume that we're given a number of particles to use $N$ and a number of steps to iterate for $k$.  We assume that we can sample uniformly from the feasible state space, $C$, which is chosen to be a rectangle in $\Bbb R^n$, and $C_v$ which is the set of original feasible speeds.  The algorithm that we used is a variant of PSO that we created specifically to cut down on expensive function evaluations and an adaption of previous variants of PSO such as \cite{carlisle}.
\\

{ \bf Binary PSO}
\begin{algorithmic}
\FOR { each particle $i$ in $1,..., N$} 
  \STATE Draw $x_i \sim C$.
  \STATE Set $p_i = x_i$.   $p_i$ is the particle's optimal point.
  \STATE Draw $v_i \sim C_v$.
\ENDFOR
\STATE Choose $p$ to be the $\text{winner}_i (p)$, so $p$ are the swarm's best point and value so far.

\FOR {iterations $j$ in $1, ..., k$}
  \FOR{ each particle $i$ in $1, ..., N$}
     \STATE Pick random numbers $r_p, r_g \sim U(0,1)$.
     \STATE Set $v_i = \omega v_i + \phi_p r_p (p_i - x_i) + \phi_g r_g (p -x_i)$.
     \STATE Set $x_i = x_i + v_i$.
     \IF {$x_i >_{agent} p_i$}
       \STATE Set $p_i=x_i$.
       \IF{$p_i >_{agent} p$}
         \STATE Set $p = p_i$.
       \ENDIF
     \ENDIF
  \ENDFOR
\ENDFOR

The parts of the algorithm that have not be fleshed out are the comparisons between points.  The operator $>_{agent}$ means that the agent on the left hand side beat the agent on the right hand side in head to head play in a statistically significant way.  The ``winner" method means selecting the ``best'' element by single matchup elimination tournament, using the $>_{agent}$ operator.
\end{algorithmic}

\subsection{Performance and Results}
Overall, we found that on the standard capture map, agents whose weights were selected by PSO tended to perform slightly worse than our hand-optimized agent, tying 61.5\% of the time, losing 30.7\% of the time, and only winning 7.7\% of the time.  In determining the reason for this apparent failure of PSO, we determined that for an average variable optimized by PSO, across all different maps we attempted, the standard deviation was 29.8\% of the possible range, and 39.8\% of the observed range.  In other words, PSO returned very different optimized results for different layouts.  Indeed, PSO won 38.5\% of the time, tied 46.1\% of the time, and lost 15.4\% of the time on the map for which it had been optimized.

\section{$k$-means Clustering for Layouts}
To solve this problem of disparate weights selected by PSO for different layouts, given a layout, we attempted to determine similar layouts for which we had already computed training data.  The idea was that using the single ``closest'' layout would be a mistake since each of our weights were computed by PSO and were thus subject to noise; averaging independent results therefore seems likely to result in better performance.

\subsection{Methodology}
We trained 20 different agents on each of the contest layouts.  Additionally we extracted some features from each board - such as amount of food, size and number of agents.  Then we used $k$-means clustering to choose a cluster similar to our observed layout.  We chose $k$-means as clustering can stabilize disparate data \cite{textbook} and we consider $k$-means the default for problems without a prior \cite{kmeans}.  Moreover the the centroid of a set of points is the natural choice for how we would mix a set of heuristics, and $k$-means matches the same metric.

When given a new board, our agents chose the cluster it belongs to and played with the average weights in its cluster.

\subsection{Performance and Results}
Overall, clustering significantly improved the efficacy of our PSO-optimized weights.  In comparison to our hand-optimized agent, among games that one of the two agents won, our averaged PSO-weighted agent won approximately 64\% of the games.  The data suggests that clustering was able to mitigate the damage of optimizing to the wrong board, and allow us to better approximate an optimal weighting.

\section{Discussion}
Our approach to Pac-Man consisted of a mix of human-determined approaches and machine learning.  We began with a collection of agents that were primarily human-optimzied, using pre-determined weights on a subset of the heuristics described above for evaluation on a Minimax tree.  These agents was only marginally more successful than the reflex agents, and had several key shortcomings; namely, that they were unable to coordinate -- to choose offense or defense -- and lacked optimal parameters.  We solved the first problem initially by assigning weights to a series of board meta-analysis conditions to switch between agent recommendations.  This substantially improved the robustness of our player, but our hand-optimized weights were still quite weak.  Our inital approach to use binary Particle Swarm Optimization to optimize these parameters was ineffective in practice, because PSO tended to give heavily board-specific weights with high variance between boards.  Applying $k$-means clustering was effective in concert with PSO to ensure that the weights suggested by our particle swarm were effectively applied to a given layout.

Overall, our agent's strongest asset is its flexibility -- a result of a wide variety of heuristics generated from human analysis of the game, PSO to develop correct weightings across different maps, and clustering to determine the proper gametime weighting.  Continued improvement could be achieved by using standard PSO, rather than binary PSO, assuming we had some objective function to minimize.  Such a function would likely be an objective measure of percentage success against a static cross-section of agents, such as repeated participation in a nightly tournament.  Additionally, running PSO and $k$-means on a larger set of layouts would likely give our agent more flexibility and encourage better performance on nonstandard layouts.


% This is the Bibliography
%%%%%%%%%%%%%%%

\begin{thebibliography}{99}

\bibitem{carlisle} Carlisle, A., Dozier, G. (2001). ``An Off-The-Shelf PSO''.
  {\it Proceedings of the Particle Swarm Optimization Workshop}. pp. 1-6. 
  
\bibitem{montecarlo} Hauskrecht, A., Singliar, T. (2003). ``Monte-Carlo optimizations for resource allocation problems in stochastic network systems".
  
\bibitem{originalref} Kennedy, J.; Eberhart, R. (1995). ``Particle Swarm Optimization". 
  {\it Proceedings of IEEE International Conference on Neural Networks. IV.} pp. 1942-1948.

\bibitem{kmeans} MacQueen, J. B. (1967). "Some Methods for classification and Analysis of Multivariate Observations". {\it Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability.} University of California Press. pp. 281Ð297.

\bibitem{miik} Miikkulainen, R. (2006) ``Creating Intelligent
  Agents in Games''.  {\it The Bridge}.  National Academy of Engineering.

\bibitem{pars} Parsopoulos, K. E., Vrahatis, M. N. (2001) Particle Swarm Optimizer in Noisy and Continuously Changing Environments. {\it Proceeding of the IASTED International Conference on Artificial Intelligence and Soft Computing} (ASC 2001), Cancun, Mexico. pp. 289-294.
  
\bibitem{geneticalgorithm} Rocha, M., Neves J. (1999) ``Preventing premature convergence to local optima in genetic algorithms via random offspring generation''. {\it IEA/AIE '99 Proceedings of the 12th international conference on Industrial and engineering applications of artificial intelligence and expert system}.

\bibitem{textbook} Russell, S., Norvig, P. (2010) ``Artificial Intelligence''. {\it A Modern Approach Third Edition} , Prentice Hall.

\end{thebibliography}

\end{document}
